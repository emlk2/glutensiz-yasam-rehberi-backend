"""
NLP Analyzer - Gluten risk analizi
"""
from typing import Dict, List, Any, Tuple
from pathlib import Path
import sys
import re

sys.path.insert(0, str(Path(__file__).parent.parent))

from db.database import db
from utils.logger import logger

try:
    from transformers import pipeline
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False


class NLPAnalyzer:
    """NLP ile gluten risk analizi"""
    
    def __init__(self):
        """NLP Analyzer'Ä± baÅŸlat"""
        self.dangerous_ingredients = db.get_dangerous_ingredients()
        self.risky_keywords = db.get_risky_keywords()
        self.classifier = None
        
        logger.info(f"ðŸ“Š NLP Analyzer baÅŸlatÄ±ldÄ±")
        logger.info(f"   âš ï¸  Tehlikeli malzeme: {len(self.dangerous_ingredients)}")
        logger.info(f"   ðŸŸ¡ Riskli kelime: {len(self.risky_keywords)}")
        
        # Transformers yÃ¼kle (opsiyonel)
        if HAS_TRANSFORMERS:
            try:
                logger.debug("ðŸ¤– Hugging Face model yÃ¼kleniyor...")
                # Zero-shot classification modeli
                self.classifier = pipeline(
                    "zero-shot-classification",
                    model="distilbert-base-multilingual-cased"
                )
                logger.info("âœ… NLP model hazÄ±r")
            except Exception as e:
                logger.warning(f"âš ï¸  NLP model yÃ¼klenemedi: {str(e)}")
                self.classifier = None
    
    def analyze_ingredients(self, ingredients_list: List[str]) -> Dict[str, Any]:
        """
        Malzemeleri analiz et
        
        Args:
            ingredients_list: Malzeme listesi
        
        Returns:
            Analiz sonuÃ§larÄ±
        """
        if not ingredients_list:
            return {
                "risk_level": "safe",
                "gluten_found": False,
                "detected_ingredients": [],
                "explanation": "Malzeme bulunamadÄ±",
                "confidence": 1.0
            }
        
        try:
            detected_ingredients = []
            has_dangerous = False
            has_risky = False
            confidence_scores = []
            
            # Her malzemeyi kontrol et
            for ingredient in ingredients_list:
                ingredient_lower = ingredient.lower().strip()
                
                # 1. KURAL TABANLI KONTROLÄ°ZE
                # Tehlikeli malzemeleri kontrol et
                for dangerous in self.dangerous_ingredients:
                    if dangerous.lower() in ingredient_lower:
                        detected_ingredients.append({
                            "ingredient": ingredient,
                            "risk_level": "dangerous",
                            "confidence": 0.99,
                            "reason": "Gluten iÃ§eren malzeme"
                        })
                        has_dangerous = True
                        confidence_scores.append(0.99)
                        break
                else:
                    # Riskli kelimeleri kontrol et
                    for risky in self.risky_keywords:
                        if risky.lower() in ingredient_lower:
                            detected_ingredients.append({
                                "ingredient": ingredient,
                                "risk_level": "risky",
                                "confidence": 0.85,
                                "reason": "Ã‡apraz bulaÅŸ veya belirsiz malzeme"
                            })
                            has_risky = True
                            confidence_scores.append(0.85)
                            break
            
            # 2. BAÄžLAMSAL ANALIZ (NLP MODEL)
            # EÄŸer model varsa ve tehlikeli malzeme bulunmadÄ±ysa, daha derinlemesine analiz yap
            if self.classifier and not has_dangerous and ingredients_list:
                try:
                    # Gluten ile ilgili malzemelerin sÄ±nÄ±flandÄ±rÄ±lmasÄ±
                    gluten_related_labels = ["contains gluten", "gluten-free", "uncertain"]
                    
                    # Her malzemeyi sÄ±nÄ±flandÄ±r
                    for ingredient in ingredients_list:
                        if ingredient not in [d.get("ingredient") for d in detected_ingredients]:
                            results = self.classifier(
                                ingredient,
                                gluten_related_labels,
                                multi_class=False
                            )
                            
                            # En yÃ¼ksek score'u al
                            top_label = results["labels"][0]
                            top_score = results["scores"][0]
                            
                            if "gluten" in top_label.lower() and top_score > 0.7:
                                detected_ingredients.append({
                                    "ingredient": ingredient,
                                    "risk_level": "risky",
                                    "confidence": round(top_score, 3),
                                    "reason": f"NLP analiz: {top_label}"
                                })
                                confidence_scores.append(top_score)
                
                except Exception as e:
                    logger.warning(f"âš ï¸  NLP model analizi baÅŸarÄ±sÄ±z: {str(e)}")
            
            # Risk seviyesini belirle
            if has_dangerous:
                overall_risk = "dangerous"
                explanation = "Gluten iÃ§eren malzeme tespit edildi!"
            elif has_risky:
                overall_risk = "risky"
                explanation = "Ã‡apraz bulaÅŸ riski veya belirsiz malzeme mevcut"
            else:
                overall_risk = "safe"
                explanation = "Gluten iÃ§eren malzeme tespit edilmedi"
            
            # Ortalama gÃ¼ven oranÄ±
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 1.0
            
            return {
                "risk_level": overall_risk,
                "gluten_found": has_dangerous,
                "cross_contamination_risk": has_risky,
                "detected_ingredients": detected_ingredients,
                "explanation": explanation,
                "confidence": round(avg_confidence, 3),
                "recommendations": self._get_recommendations(overall_risk)
            }
        
        except Exception as e:
            logger.error(f"âŒ Analiz hatasÄ±: {str(e)}", exc_info=True)
            return {
                "risk_level": "unknown",
                "gluten_found": False,
                "error": str(e),
                "explanation": "Analiz yapÄ±lamadÄ±"
            }
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        Metin iÃ§erisinde gluten ara (OCR'dan gelen metin)
        
        Args:
            text: GÃ¶rÃ¼ntÃ¼den Ã§Ä±karÄ±lan metin
        
        Returns:
            Analiz sonuÃ§larÄ±
        """
        if not text:
            return {
                "risk_level": "safe",
                "gluten_found": False,
                "explanation": "Metin boÅŸ"
            }
        
        try:
            text_lower = text.lower()
            
            # Tehlikeli kelimeleri ara
            dangerous_count = 0
            dangerous_found = []
            
            for ingredient in self.dangerous_ingredients:
                if ingredient.lower() in text_lower:
                    dangerous_count += 1
                    dangerous_found.append(ingredient)
            
            # Riskli kelimeleri ara
            risky_count = 0
            risky_found = []
            
            for keyword in self.risky_keywords:
                if keyword.lower() in text_lower:
                    risky_count += 1
                    risky_found.append(keyword)
            
            # Risk belirle
            if dangerous_count > 0:
                risk_level = "dangerous"
                explanation = f"Gluten iÃ§eren malzeme bulundu: {', '.join(dangerous_found)}"
            elif risky_count > 0:
                risk_level = "risky"
                explanation = f"Riskli ifadeler: {', '.join(risky_found)}"
            else:
                risk_level = "safe"
                explanation = "Gluten iÃ§eren malzeme bulunamadÄ±"
            
            return {
                "risk_level": risk_level,
                "gluten_found": dangerous_count > 0,
                "cross_contamination_risk": risky_count > 0,
                "dangerous_ingredients_found": dangerous_found,
                "risky_keywords_found": risky_found,
                "explanation": explanation,
                "recommendations": self._get_recommendations(risk_level)
            }
        
        except Exception as e:
            logger.error(f"âŒ Metin analizi hatasÄ±: {str(e)}")
            return {
                "risk_level": "unknown",
                "error": str(e)
            }
    
    def _get_recommendations(self, risk_level: str) -> List[str]:
        """Risk seviyesine gÃ¶re tavsiyeleri dÃ¶ndÃ¼r"""
        recommendations = {
            "safe": [
                "âœ… Bu Ã¼rÃ¼n gluten iÃ§ermediÄŸi gÃ¶rÃ¼lmektedir",
                "ðŸ“Œ Yine de Ã¼retici bilgilerini kontrol etmeniz Ã¶nerilir",
                "ðŸ’¡ Sertifikasyonu varsa tercih etmeyi dÃ¼ÅŸÃ¼nÃ¼n"
            ],
            "risky": [
                "âš ï¸  Ã‡apraz bulaÅŸ riski olabilir",
                "ðŸ“ž ÅžÃ¼pheli durumlarda Ã¼reticiyi arayÄ±n",
                "ðŸ“– Dernek rehberinde kontrol etmeyi deneyin",
                "ðŸ’¡ Hassas iseniz bu Ã¼rÃ¼nÃ¼ tercih etmeyin"
            ],
            "dangerous": [
                "âŒ Bu Ã¼rÃ¼n gluten iÃ§ermektedir - RISKLI",
                "ðŸš« Ã‡Ã¶lyak hastasÄ± olarak TÃœKETMEYÄ°N",
                "ðŸ“ž ÅžÃ¼pheyi doÄŸrulamak iÃ§in Ã¼reticiyi arayabilirsiniz",
                "ðŸ’¡ Alternativ Ã¼rÃ¼nler seÃ§in"
            ]
        }
        
        return recommendations.get(risk_level, [])
    
    def calculate_risk_score(self, analysis_result: Dict[str, Any]) -> float:
        """
        Risk puanÄ± hesapla (0-1, 1 en riskli)
        """
        risk_scores = {
            "safe": 0.0,
            "risky": 0.5,
            "dangerous": 1.0,
            "unknown": 0.3
        }
        
        base_score = risk_scores.get(analysis_result.get("risk_level"), 0.5)
        confidence = analysis_result.get("confidence", 0.5)
        
        # GÃ¼ven oranÄ± ne kadar dÃ¼ÅŸÃ¼kse, risk puanÄ± artar
        adjusted_score = base_score * (1 + (1 - confidence) * 0.2)
        
        return round(min(adjusted_score, 1.0), 3)


# Global NLP instance
nlp_analyzer = None

def get_nlp_analyzer() -> NLPAnalyzer:
    """NLP analyzer'Ä± lazily yÃ¼kle"""
    global nlp_analyzer
    if nlp_analyzer is None:
        nlp_analyzer = NLPAnalyzer()
    return nlp_analyzer
